---
title: "Effectiveness of using Dietary Composition to predict Type II Diabetes and Dyslipidemia for ages 18+"
author: "Ian Astalosh, Cassie Brooks, Catherine Chen, Jerry Xu, Leon Yao"
output: pdf_document
---

```{r, include=FALSE}

#PACKAGES
#install.packages(ggplot2)
#install.packages()
#install.packages()
#library(ggplot2)
#library()
#library()

```


# Executive Summary
This collaborative effort between nutrition and statistics departments aims to assess the impact of dietary fats on non-communicable diseases (NCD's). The data was taken from the Australian Health Survey 2011-2012 (AHS), at the time the largest health survey ever conducted in Australia. Focussing on adults (ages 18 and over), the main research questions we sought to answer were:

\begin{enumerate}
\item What factors influence the chances of a person having an NCD such as type II diabetes or dyslipidemia?
\item How does the amount of dietary fat affect a person's chances of having type II diabetes or dyslipidemia?
\item How do free sugars and added sugars affect a person's chances of having type II diabetes or dyslipidemia?
\item Is it possible to predict prevalence of these diseases from only basic physical details and these dietary breakdowns?
\end{enumerate}

We found \textbf{results}

# Context

```{r, echo=FALSE, include=FALSE}

#PACKAGES USED IN THIS ANALYSIS. TO INSTALL SIMPLY UNCOMMENT THE LINE.

# install.packages("tidyverse")
# install.packages("tidyr")
# install.packages("dplyr")
# install.packages("broom") # for tidy()
# install.packages("ggplot2")
# install.packages("pROC")
# install.packages("plotROC")
# install.packages("gmodels") #CrossTable
# install.packages("MLmetrics")
# install.packages("ROCR")
# install.packages("e1071") # SVM
# install.packages("pscl")
# install.packages("caTools") # train_test_split
# install.packages("gridExtra") # Grid plotting for ggplot2
# install.packages("factoextra")
# install.packages("kmed")
# install.packages("cluster")
# install.packages("outliers")
# install.packages("ggbiplot")
# # install.packages("rsample")      # data splitting - not available for the most recent version of R
# install.packages("randomForestSRC") # basic RF package
# install.packages("ranger")
# install.packages("kableExtra")
# install.packages("kable")
# install.packages("reshape2")

suppressMessages(library(tidyverse))
library(tidyr)
library(dplyr)
library(broom) # for tidy()
library(ggplot2)
library(pROC)
library(plotROC)
library(gmodels) #CrossTable
library(MLmetrics)
library(ROCR)

library(e1071) # SVM

library(pscl)
library(caTools) # train_test_split

library(gridExtra) # Grid plotting for ggplot2
library(factoextra)
library(kmed)
library(cluster)
library(outliers)
library(ggbiplot)

# library(rsample)      # data splitting - not available for the most recent version of R
library(randomForestSRC) # basic RF package
library(ranger)
library(kableExtra)
library(reshape2)

source("./functions/cv_knn.R")
source("./functions/cv_da.R")
source("./functions/cv_rpart.R")
source("./functions/cv_glm.R")
source("./functions/cv_glm_backward.R")

feature_variables = c("BMISC", "AGEC", "PHDKGWBC", "EXLWTBC", "SF2SA1QN", "INCDEC", "HSUGBC", "FATT1", "SUGART1", 
                      "PREVAT1", "PROVAT1", "FATPER1", "LAPER1", "ALAPER1", "CHOPER1", "SUGPER1", "SATPER1", 
                      "TRANPER1", "MONOPER1", "POLYPER1", "ADTOTSE", "SEX", "SMKSTAT", "SYSTOL", "FASTSTAD", 
                      "HDLCHREB", "LDLNTR", "LDLRESB", "B3T1",
                      
                      "CHOWSAT1", "STARCHT1", "FIBRET1", "FIBRPER1", "ALCT1", "ALCPER1", 
                      "PEFRESD1", "PEADDSD1")
# BMR, SLPTIME
response_variables = c("CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST")
all_variables = c(feature_variables, response_variables)
nutm_orig = read.csv("../output/nutmstat_factors_and_NAs.csv")
nutm = nutm_orig[all_variables]

```

We'll need to reprocess some of the variables as factors. 

```{r, include=FALSE}

categoricalList <- c()
categoricalList[ 1 ] <- FALSE #  BMISC 
categoricalList[ 2 ] <- FALSE #  AGEC 
categoricalList[ 3 ] <- FALSE #  PHDKGWBC 
categoricalList[ 4 ] <- FALSE #  EXLWTBC 
categoricalList[ 5 ] <- TRUE #  SF2SA1QN 
categoricalList[ 6 ] <- TRUE #  INCDEC 
categoricalList[ 7 ] <- TRUE #  HSUGBC 
categoricalList[ 8 ] <- FALSE #  FATT1 
categoricalList[ 9 ] <- FALSE #  SUGART1 
categoricalList[ 10 ] <- FALSE #  PREVAT1 
categoricalList[ 11 ] <- FALSE #  PROVAT1 
categoricalList[ 12 ] <- FALSE #  FATPER1 
categoricalList[ 13 ] <- FALSE #  LAPER1 
categoricalList[ 14 ] <- FALSE #  ALAPER1 
categoricalList[ 15 ] <- FALSE #  CHOPER1 
categoricalList[ 16 ] <- FALSE #  SUGPER1 
categoricalList[ 17 ] <- FALSE #  SATPER1 
categoricalList[ 18 ] <- FALSE #  TRANPER1 
categoricalList[ 19 ] <- FALSE #  MONOPER1 
categoricalList[ 20 ] <- FALSE #  POLYPER1 
categoricalList[ 21 ] <- FALSE #  ADTOTSE 
categoricalList[ 22 ] <- TRUE #  SEX 
categoricalList[ 23 ] <- TRUE #  SMKSTAT 
categoricalList[ 24 ] <- FALSE #  SYSTOL 
categoricalList[ 25 ] <- TRUE #  FASTSTAD 
categoricalList[ 26 ] <- TRUE #  HDLCHREB 
categoricalList[ 27 ] <- TRUE #  LDLNTR 
categoricalList[ 28 ] <- TRUE #  LDLRESB 
categoricalList[ 29 ] <- FALSE #  B3T1
categoricalList[ 30 ] <- FALSE #  CHOWSAT1 
categoricalList[ 31 ] <- FALSE #  STARCHT1 
categoricalList[ 32 ] <- FALSE #  FIBRET1 
categoricalList[ 33 ] <- FALSE #  FIBRPER1 
categoricalList[ 34 ] <- FALSE #  ALCT1 
categoricalList[ 35 ] <- FALSE #  ALCPER1
categoricalList[ 36 ] <- FALSE #  PERFRESD1 
categoricalList[ 37 ] <- FALSE #  PEADDSD1 
categoricalList[ 38 ] <- TRUE #  CHOLNTR 
categoricalList[ 39 ] <- TRUE #  HDLCHREB 
categoricalList[ 40 ] <- TRUE #  DIABBC 
categoricalList[ 41 ] <- TRUE #  HCHOLBC 
categoricalList[ 42 ] <- TRUE #  HYPBC 
categoricalList[ 43 ] <- TRUE #  CVDMEDST 
# "CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST"
for (i in 1:length(categoricalList)) {
  if (categoricalList[ i ]) {
      nutm[,i] <- as.factor(nutm[ ,i])
  } else {
     nutm[, i] <- as.numeric(nutm[, i])
  }
}
head(nutm)

```

# Exploratory Data Analysis

## Missing Values
The data supplied for this project from the AHS contained 12,153 instances of 144 variables. By considering only adults (ages 18+) and consulting with the nutrition students on what variables to include, our working dataset was trimmed to 9435 instances of 44 variables.  

The data firstly needed to be cleaned as there were large amounts of missingness. Our response variables of interest are DIABBC (presence of type 2 diabetes) and CVDMEDST (dyslipidemia). For variables that were identified as potential response variables, the missingness actually varied significantly:

```{r, echo=FALSE, fig=TRUE}
diabbc_miss = 100*mean(is.na(nutm$DIABBC))
dyslip_miss = 100*mean(is.na(nutm$CVDMEDST))

miss_mat = matrix(c(diabbc_miss, dyslip_miss), nrow=1)
colnames(miss_mat) = c("Diabetes", "Dyslipidemia"); rownames(miss_mat) = "Percentage Data Missing"

kabtab = kable(miss_mat)
kable_styling(kabtab)

```

We have complete responses for diabetes, however only about a quarter of respondents for dyslipidemia. We deal with missing data by only keeping the complete cases. We considered using multiple imputation methods, however the data loss is not too bad.

## Dietary Sugars and Fats
We now examine the variables we are most interested in as predictors. Specifically, we plot the data grouped by the presence or absence of the particular disease in question.

```{r, include=FALSE, echo=FALSE}
#DYSLIP
dyslipidemia_response = c("CVDMEDST")
dyslipidemia_variables = c("BMISC", "AGEC", "EXLWTBC", 
                       "SUGART1", "CHOPER1", 
                       "FATPER1", "LAPER1", "ALAPER1", 
                       "SATPER1", "TRANPER1", "MONOPER1", "POLYPER1", 
                       "ADTOTSE",
                       "SYSTOL", "B3T1",
                       "CHOWSAT1", "STARCHT1", "FIBRPER1", "ALCPER1",     
                       "PEFRESD1", "PEADDSD1"
                       )
dyslipidemia = nutm[c(dyslipidemia_response, dyslipidemia_variables)]
head(dyslipidemia)
dyslipidemia <- mutate(dyslipidemia, CVDMEDST = ifelse(CVDMEDST == 4, "0", "1"))
dyslipidemia$CVDMEDST <- as.factor(dyslipidemia$CVDMEDST)

dyslipidemia_no_na <- dyslipidemia[complete.cases(dyslipidemia), ]

dim(dyslipidemia_no_na)
head(dyslipidemia_no_na)
table(dyslipidemia_no_na$CVDMEDST)


diabetes_response = c("DIABBC")
diabetes_variables = c("BMISC", "AGEC", "EXLWTBC", 
                       "SUGART1", "CHOPER1", 
                       "FATPER1", "LAPER1", "ALAPER1", 
                       "SATPER1", "TRANPER1", "MONOPER1", "POLYPER1", 
                       "ADTOTSE",
                       "SYSTOL", "B3T1",
                      "CHOWSAT1", "STARCHT1", "FIBRPER1", "ALCPER1", 
                      "PEFRESD1", "PEADDSD1"
                       )

diabetes = nutm[c(diabetes_response, diabetes_variables)]
diabetes = diabetes[diabetes$DIABBC != 3, ]

table(diabetes$DIABBC)

diabetes <- mutate(diabetes, DIABBC = ifelse(DIABBC == 5, "0", "1"))
diabetes$DIABBC <- as.factor(diabetes$DIABBC)

diabetes_no_na <- diabetes[complete.cases(diabetes), ]

dim(diabetes_no_na)
head(dyslipidemia_no_na)
table(diabetes_no_na$DIABBC)


```

```{r, echo=FALSE, fig=TRUE, message=FALSE}
dyslipfats1 = dyslipidemia_no_na[,c("CVDMEDST","LAPER1","SATPER1","MONOPER1","POLYPER1")]
dyslipfats2 = dyslipidemia_no_na[,c("CVDMEDST","ALAPER1", "TRANPER1")]

dyslipsugars1 = dyslipidemia_no_na[, c("CVDMEDST","PEFRESD1", "PEADDSD1")]

melted_dyslip_fats1 = melt(dyslipfats1)
colnames(melted_dyslip_fats1) = c("Has Dyslipidemia", "Variable", "Value")
fat1 = ggplot(melted_dyslip_fats1, aes(x = Variable, y = Value, fill = `Has Dyslipidemia`)) + geom_boxplot() + ggtitle("Percentage of Energy Intake from Fats") + theme(plot.title = element_text(hjust = 0.5, size=9), axis.title.x = element_text(size=8), axis.title.y = element_text(size=8), axis.text.x = element_text(size=5), legend.title = element_text(size=8))

melted_dyslip_fats2 = melt(dyslipfats2)
colnames(melted_dyslip_fats2) = c("Has Dyslipidemia", "Variable", "Value")
fat2 = ggplot(melted_dyslip_fats2, aes(x = Variable, y = Value, fill = `Has Dyslipidemia`)) + geom_boxplot() + ggtitle("Percentage of Energy Intake from Fats") + theme(plot.title = element_text(hjust = 0.5, size=9), axis.title.x = element_text(size=8), axis.title.y = element_text(size=8), axis.text.x = element_text(size=5), legend.title = element_text(size=8))

melted_dyslip_sugars1 = melt(dyslipsugars1)
colnames(melted_dyslip_sugars1) = c("Has Dyslipidemia", "Variable", "Value")
sugar1 = ggplot(melted_dyslip_sugars1, aes(x = Variable, y = Value, fill = `Has Dyslipidemia`)) + geom_boxplot() + ggtitle("Percentage of Energy Intake from Sugars") + theme(plot.title = element_text(hjust = 0.5, size=9), axis.title.x = element_text(size=8), axis.title.y = element_text(size=8), axis.text.x = element_text(size=5), legend.title = element_text(size=8))

grid.arrange(fat1, fat2, sugar1, nrow=2)
```
There are a few things to note here. Firstly, the data is skewed with a large number of outliers. With more time we would have liked to spend more time examining outliers to determine which are valid for the analysis, however we did not. Secondly, we note there does not appear to be significant difference between these predictors, which suggests predicting dyslipidemia will be difficult. 

(boxplots of fats with and without diabetes, dyslipidemia)
(boxplots of sugars with and without diabetes, dyslipidemia)

(correlations among all factors)


# Models

Although we directed our focus towards the relationship between sugars and diabetes/dyslipidemia, we also looked at the relationship between various fats to cholesterol levels as well, increasing our analysis scope to 6 response variables. 

The response variables analysed fall into 3 main categories. The diabetes response variable (DIABBC), cholesterol levels (CHOLNTR), LDL cholesterol variables (LDLNTR) and dyslipidemia (CVDMEDST), are all binary outcomes, and thus we can use various straightforward classification models for prediction. For DIABBC, the imbalance between the 2 classes required upsampling in order to achieve an effective balance for learning. We used a straight upsampling method and a more complicated SMOTE sampling method, that replicates the data by changing it slightly using a nearest neighbours approach.

The other 2 response variables, HDL cholesterol (HDLCHREB) and LDL cholesterol (LDLRESB) are all ordinal, categorical variables the represent a range of mmol/L concentrations from low to high. Analysis with these variables are conducted with proportional odds models (or an ordered logistic regression).

The discussion begins with the less successful modelling outcomes for the binary response variables, namely LDLNTR and CHOLNTR. Despite utilising a variety of classification algorithms and hyperparameter tuning techniques, the outcome of the models were at best a random guess. This is due to the feature variables being indistinguishable between the 2 classes, resulting in a significant overlap in the n-dimensional hyperplane, making it impossible of the algorithms to learn the unique features of each class for classification. 

At the end of the all the analysis, we visualised the densities of each feature variable grouped by class and unsurprisingly, the densities were almost identical in most cases except for the age variable. We further confirmed this by running a PCA to visualise the variabilities of the data on a 2D plan by class, and again, the classes were spread evenly across the 2 most important components. This meant that there was variance in the variables, but unfortunately the distinction between the 2 classes were insufficient for modelling.

In analysing diabetes, we were able to achieve an acceptable FNR & FPR trade-off of $\approx$30% respectively after tuning the probability threshold hyperparameter for the logistic regression model. The random forest model evaluated blood pressure and sedentary activity, along with percentage fibre intake and added sugars as the most important variables, and this was

```{r}

sample_sub = sample.split(diabetes_no_na$DIABBC, SplitRatio = .7)
train_diabetes = subset(diabetes_no_na, sample_sub == TRUE)
test_diabetes  = subset(diabetes_no_na, sample_sub == FALSE)

SMOTE_data <- SMOTE(train_diabetes[, -1], train_diabetes[, 1], 
      dup_size=10, K = 7)

names(SMOTE_data$data)[names(SMOTE_data$data) == 'class'] <- "DIABBC"

probs_tuning <- seq(0.05, 0.95, by=0.05)

length(probs_tuning)

accuracy <- c()
fpr <- c()
fnr <- c()
f1 <- c()

for (i in 1:length(probs_tuning) ){
  
    test_tuning_logit <- predict(logistic_regression, newdata=test_diabetes, type='response')
    test_tuning_logit <- ifelse(test_tuning_logit > probs_tuning[i], 1, 0)
    
    table_results <- table(test_diabetes$DIABBC, test_tuning_logit)
    
    #print(paste("Probability threshold: ", probs_tuning[i]))
    #print(table_results)
    
    accuracy[i] <- (table_results[1] + table_results[4])/sum(table_results)
    fpr[i] <- table_results[3]/(table_results[1] + table_results[3])
    fnr[i] <- table_results[2]/(table_results[2] + table_results[4])
    #f1[i] <- F1_Score(test$CVDMEDST, test_tuning_logit, positive=1) 
}

tuning_results <- data.frame(cbind(probs_tuning, "accuracy" = accuracy*100, "FPR" = fpr*100, "FNR" = fnr*100, "F1" = f1))
tuning_results

ggplot(tuning_results, aes(x=fnr, y=fpr)) + geom_point(colour="red", size=2.5) + xlab("False Positive Rate %") + ylab("False Negative Rate %")

```


We will use a number of classifiers to attempt to predict which patients have the non-communicable diseases of high cholsterol, type II diabetes, hypertension and dyslipidemia. We used a number of classifiers, however the two main classifiers we focus on are generalised linear models (logistic regressions) and random forests.
 
The reason for this is to take a more prediction based approach as well as a more inferential based approach. The random forest method was chosen as it is an extremely powerful classifcation algorithm, capable of handling different types of data and adept at preventing overfitting through its ensemble method. From this, we will gain highly optimized predictions of whether a person will or will not have a certain disease. However, it is limited in its scope in terms of interpretability, as it acts as somewhat of a black box model. From our random forests we will see which variables are most significant in determining the classification, but with no indication as to whether they are significant in a positive or negative way, for example.

Our logistic regression, alternatively, provides greater insight into the effect of each variable through its parameter estimates. While the error may be higher, we will more clearly be able to see the size and impact of the effect of each covariate. Specifically, this will enable us to examine which dietary fats positively and negatively impact a person's chances of disease.

We additionally fitted other classifiers such as KNN and CART for reference, however we are most interested in the GLMs and random forests. 



# Discussion




# References