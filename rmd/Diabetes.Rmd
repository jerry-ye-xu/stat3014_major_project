---
title: "Diabetes.Rmd"
author: "Jerry Xu"
date: "16 October 2018"
output: html_document
---

## Predicting Diabetes Mellitus

Warning: The glm functions take a while to run. They have been commented out to save time when running the whole file

```{r, include=TRUE}

# library(tidyverse)
library(tidyr)
library(dplyr)
library(broom) # for tidy()
library(ggplot2)
library(pROC) # AUC and POC
library(plotROC) 
library(gmodels) #CrossTable
library(e1071) # SVM

# library(rsample)      # data splitting - not available for the most recent version of R
library(randomForestSRC) # basic RF package
library(ranger)

source("./functions/cv_knn.R")
source("./functions/cv_da.R")
source("./functions/cv_rpart.R")
source("./functions/cv_glm.R")
source("./functions/cv_glm_backward.R")

```

## Standardised Preprocessing
 
```{r}

feature_variables = c("BMISC", "AGEC", "PHDKGWBC", "EXLWTBC", "SF2SA1QN", "INCDEC", "HSUGBC", "FATT1", "SUGART1", 
                       "PREVAT1", "PROVAT1", "FATPER1", "LAPER1", "ALAPER1", "CHOPER1", "SUGPER1", "SATPER1", "TRANPER1", 
                       "MONOPER1", "POLYPER1", "ADTOTSE", "SEX", "SMKSTAT", "SYSTOL", "FASTSTAD", "HDLCHREB", "LDLNTR", "LDLRESB", "B3T1")

# BMR, SLPTIME

response_variables = c("CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST")

all_variables = c(feature_variables, response_variables)

nutm_orig = read.csv("../output/nutmstat_factors_and_NAs.csv")
nutm = nutm_orig[all_variables]

```

```{r}

table(nutm$CHOLNTR) # Total Cholesterol Status
table(nutm$HDLCHREB) # HDL Cholesterol Range
table(nutm$LDLRESB) # LDL Cholesterol Range
table(nutm$DIABBC) # Diabetes Mellitus status
table(nutm$HCHOLBC) # High Cholesterol Status 
table(nutm$HYPBC) # Hypertensive Disease status
table(nutm$CVDMEDST) # Dyslipidemia status (1-3 are similar)

```

We'll need to reprocess some of the variables as factors. 

```{r}
categoricalList <- c()

categoricalList[ 1 ] <- FALSE #  BMISC 
categoricalList[ 2 ] <- FALSE #  AGEC 

categoricalList[ 3 ] <- FALSE #  PHDKGWBC 

categoricalList[ 4 ] <- FALSE #  EXLWTBC 

categoricalList[ 5 ] <- TRUE #  SF2SA1QN 
categoricalList[ 6 ] <- TRUE #  INCDEC 

categoricalList[ 7 ] <- TRUE #  HSUGBC 

categoricalList[ 8 ] <- FALSE #  FATT1 

categoricalList[ 9 ] <- FALSE #  SUGART1 

categoricalList[ 10 ] <- FALSE #  PREVAT1 
categoricalList[ 11 ] <- FALSE #  PROVAT1 

categoricalList[ 12 ] <- FALSE #  FATPER1 
categoricalList[ 13 ] <- FALSE #  LAPER1 
categoricalList[ 14 ] <- FALSE #  ALAPER1 
categoricalList[ 15 ] <- FALSE #  CHOPER1 
categoricalList[ 16 ] <- FALSE #  SUGPER1 

categoricalList[ 17 ] <- FALSE #  SATPER1 
categoricalList[ 18 ] <- FALSE #  TRANPER1 

categoricalList[ 19 ] <- FALSE #  MONOPER1 
categoricalList[ 20 ] <- FALSE #  POLYPER1 
categoricalList[ 21 ] <- FALSE #  ADTOTSE 

categoricalList[ 22 ] <- TRUE #  SEX 

categoricalList[ 23 ] <- TRUE #  SMKSTAT 
categoricalList[ 24 ] <- FALSE #  SYSTOL 

categoricalList[ 25 ] <- TRUE #  FASTSTAD 

categoricalList[ 26 ] <- TRUE #  HDLCHREB 
categoricalList[ 27 ] <- TRUE #  LDLNTR 
categoricalList[ 28 ] <- TRUE #  LDLRESB 

categoricalList[ 29 ] <- FALSE #  B3T1

categoricalList[ 30 ] <- TRUE #  CHOLNTR 
categoricalList[ 31 ] <- TRUE #  HDLCHREB 
categoricalList[ 32 ] <- TRUE #  DIABBC 
categoricalList[ 33 ] <- TRUE #  HCHOLBC 
categoricalList[ 34 ] <- TRUE #  HYPBC 
categoricalList[ 35 ] <- TRUE #  CVDMEDST 

# "CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST"

for (i in 1:length(categoricalList)) {
  if (categoricalList[ i ]) {
      nutm[,i] <- as.factor(nutm[ ,i])
  }
}

head(nutm)
```

#### Preprocessing Data Stage 

We use to model the "DIABBC" variable. It contains 3 outcomes:

```{r}

table(nutm["DIABBC"])

```

\begin{tabular}{|c|c|}
\hline
Outcome & Interpretation    \\
\hline
1       & Currently has DB  \\
3       & Had DB previously \\
5       & Does not have DB  \\
\hline
\end{tabular}

Judging by the distribution of the different responses, we may have to drop 3 entirely as the definition of such an outcome is unclear. We do not know how long ago they stopped, although a retrospective study of their health improvements could be an interesting further piece of work.

Given the disparity between the binary outcomes, a Poisson model here makes sense. 

Now we need to select our feature variables. 

```{r}
diabetes_response = c("DIABBC")
diabetes_variables = c("BMISC", "AGEC", "EXLWTBC", 
                       # "SF2SA1QN", "INCDEC",
                       # "HSUGBC", 
                       "FATT1", "SUGART1", 
                       "FATPER1", "LAPER1", "ALAPER1", "CHOPER1", 
                       "SATPER1", "TRANPER1", "MONOPER1", "POLYPER1", 
                       "ADTOTSE",
                       # "SEX"
                       # "SMKSTAT", 
                       "SYSTOL" 
                       # "FASTSTAD", 
                       # "HDLCHREB", "LDLNTR", "LDLRESB", 
                       #"B3T1"
                       )

diabetes = nutm[c(diabetes_response, diabetes_variables)]
diabetes = diabetes[diabetes$DIABBC != 3, ]

table(diabetes$DIABBC)

diabetes <- mutate(diabetes, DIABBC = ifelse(DIABBC == 5, "0", "1"))
diabetes$DIABBC <- as.factor(diabetes$DIABBC)
head(diabetes)
```

```{r}

sum(is.na(nutm[diabetes_response]))

diabetes_no_na <- diabetes[complete.cases(diabetes), ]

dim(diabetes_no_na)
table(diabetes_no_na$DIABBC)

```

This is a huge disproportion between the 2 outcomes. 

```{r}

7343/(7343+471)

```
With a null accuracy of ~94%, it is going to be difficult to use the standard classification algorithms to acheive any improvements in prediction accuracy.

Later on we will attempt to use different methods such as upsampling. This may improve our results. 

It seems like dropping all rows with any NA values does not change the binary outcomes of the response by alot. This is good. 

Setting variables:

```{r}

df_len = dim(diabetes_no_na)[2]
df_len

df_samples = dim(diabetes_no_na)[1]
df_samples

neighbours = 3
V = 10

```

Now we can try to fit a KNN to test the code. 

```{r}

X = data.frame(scale(diabetes_no_na[, 2:df_len]))
y = diabetes_no_na$DIABBC
head(X)
cv_knn(X, y, k=neighbours, V=10, seed=1)

k_neighbours = seq(1,29,by=2)
cv_errors = c()
for (i in 1:length(k_neighbours)) {
  cv_errors[i] = cv_knn(X, as.factor(y),k=k_neighbours[i],V,seed=1)
}
plot(k_neighbours, cv_errors, type="l", xlab="value of k", ylab="cross-validation error", cex.lab=1.5,
    main="CV errors for K-nearest neighbours", cex.main=2, ylim=c(0.0,0.20))

```

We quickly calculate the ROC curve here for knn here, after splitting the samples 70/30.

```{r}

train <- sample_frac(diabetes_no_na, 0.7)
train_idx <-as.numeric(rownames(train))
test <- diabetes_no_na[-train_idx, ]
head(train)
head(test)

train_X <- train[, 2:df_len]
test_X <-   test[, 2:df_len]


knn_split_test <- knn(train=train_X, test=test_X, cl=train$DIABBC , k = 5)
CrossTable(test$DIABBC, knn_split_test)

roc_knn <- roc(as.numeric(test$DIABBC), as.numeric(knn_split_test))
roc_knn
plot(roc_knn)
```

Not surprisingly, the AUC of the ROC is absolutely terrible, with a 98.8% proportion of a false negative. This is the issue with imbalanced classes. 


```{r}
X <- as.matrix(diabetes_no_na[, 2:15])
y <- diabetes_no_na$DIABBC

diabetes_no_na_cart <- data.frame(y, X)

minsplit = 5
maxdepth = 10

cv_rpart(X,y,V)

res_rpart <- rpart(y ~ X, data=diabetes_no_na_cart, method="anova", control = list(minsplit = minsplit, maxdepth = maxdepth))
res_rpart
rpart.plot(res_rpart,type=1,extra=1, main="CART for Diabetes")

```

```{r}

train <- sample_frac(diabetes_no_na_cart, 0.7)
train_idx <-as.numeric(rownames(train))
test <- diabetes_no_na_cart[-train_idx,]
head(train)
head(test)

diabetes_train <- ranger(
  formula = y ~ .,
  data    = train,
  importance = "impurity"
  # xtest   = test$y,
  # ytest   = test[, 2:df_len]
)

diabetes_train$variable.importance
diabetes_train$prediction.error

print("Predict using the test set")
diabetes_test <- predict(diabetes_train, data = test)

CrossTable(test$y, diabetes_test$predictions)

diabetes_train$variable.importance %>% 
  tidy() %>%
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 25 important variables")

```

Based on the confusion matrix, the accuracy is

```{r}

(3941+182)/(3941+182+1+89)*100

```

Accuracy is greater than the null accuracy!

However, the issue with this is that there are too many false negatives, type II errors. 

```{r}

roc <- roc(response=as.numeric(test$y), predictor=as.numeric(diabetes_test$predictions))
plot(roc)

roc

```


```{r, eval=FALSE, include=FALSE}

hyper_grid_2 <- expand.grid(
  mtry       = seq(1, 14, by = 2),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE  = 0
)

hyper_grid_2

# perform grid search
for(i in 1:nrow(hyper_grid_2)) {
  
  # train model
  model <- ranger(
    formula         = y ~ ., 
    data            = train, 
    importance      = "impurity",
    num.trees       = 500,
    mtry            = hyper_grid_2$mtry[i],
    min.node.size   = hyper_grid_2$node_size[i],
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid_2$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_2 %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)

```

### LDA and QDA

```{r}

X = data.frame(scale(diabetes_no_na[, 2:15]))
y = diabetes_no_na$DIABBC

cv_da(X = X, y = y, V = 10, method = "lda", seed = 1)
cv_da(X = X, y = y, V = 10, method = "qda", seed = 1)

```

### GLMs 

```{r}

X = data.frame(scale(diabetes_no_na[, 2:15]))
y = diabetes_no_na$DIABBC

mutate(diabetes_no_na, DIABBC = ifelse(DIABBC == "0", 0, 1))

logistic_regression = glm(y~., family=binomial, data=X)
summary(logistic_regression)

```

```{r, eval=FALSE, include=FALSE}

cv_glm(X, y, V = 10)
cv_glm_backward(X, y, V = 10, pen = log(df_samples))
cv_glm_backward(X, y, V = 10, pen = 2)

```

### Dealing with Disproportionate Data

The previous methods did not work so well as the response variable classes are extremely imbalanced. Random forests, despite the slight improvement in accuracy to 97%, still produced too many Type II errors. This may or may not be relevant depending on the problem at hand but in healthcare it may be better to have a predictor that is more conservative and allows more for false positives.

Now we consider a few methods to handle such imbalanced classes, including upsampling, penalised-SVM and outlier detection methods.

#### 1) Upsampling

We generate synthetic data by bootstrapping the minority-class samples that we have observed.

The key here is to partition the data first, ensuring the classes are even. Then we upsample the data of the training set, build the model and then evaluate using the test set that we partitioned out earlier. 

This is essential considering that the samples were bootstrapped, so technically they were already in-sample predictions. 

```{r}

train_diabetes <- sample_frac(diabetes_no_na, 0.7)
train_idx <-as.numeric(rownames(train))
test_diabetes <- train_diabetes[-train_idx,]
head(test_diabetes)

minority_class <- train_diabetes[train_diabetes$DIABBC == 1, ]
#majority_class <- train_diabetes[diabetes_no_na$DIABBC == 0, ]
minor_idx <- rownames(minority_class)
#majority_idx <- rownames(majority_class)
minor_idx[1:10]
#majority_idx

table(train_diabetes$DIABBC)

upsample_size = 2500
minor_upsample <- sample(x=minor_idx, size=upsample_size, replace=TRUE)
minor_upsample[1:10]
```

```{r}

diabetes_no_na[minor_upsample[1], ]

diabetes_upsample = train_diabetes 

for (i in 1:upsample_size){
    diabetes_upsample <- rbind(diabetes_upsample, diabetes_no_na[minor_upsample[i], ])
}

head(diabetes_no_na)
```

```{r}
tail(diabetes_upsample)
total_rows <- dim(diabetes_upsample)[1]

rownames(diabetes_upsample) <- c(seq(1:total_rows))
tail(diabetes_upsample)

table(diabetes_upsample$DIABBC)
table(test_diabetes$DIABBC)
```

The null accuracy is

```{r}

4980/(4980+2817)*100
1533/(1533+77)*100

```


```{r}

X = data.frame(scale(diabetes_upsample[, 2:df_len]))
y = diabetes_upsample$DIABBC
head(X)
cv_knn(X, y, k=neighbours, V=10, seed=1)

k_neighbours = seq(1,29,by=2)
cv_errors = c()
for (i in 1:length(k_neighbours)) {
  cv_errors[i] = cv_knn(X, as.factor(y),k=k_neighbours[i],V,seed=1)
}
plot(k_neighbours, cv_errors, type="l", xlab="value of k", ylab="cross-validation error", cex.lab=1.5,
    main="CV errors for K-nearest neighbours", cex.main=2, ylim=c(0.0,0.3))

```

```{r}
X <- as.matrix(diabetes_upsample[, 2:df_len])
y <- diabetes_upsample$DIABBC

diabetes_upsample_cart <- data.frame(y, X)

minsplit = 5
maxdepth = 10

cv_rpart(X,y,V)

res_rpart <- rpart(y ~ X, data=diabetes_upsample_cart, method="anova", control = list(minsplit = minsplit, maxdepth = maxdepth))
res_rpart
rpart.plot(res_rpart,type=1,extra=1, main="CART for Diabetes")

```

```{r}

train <- sample_frac(diabetes_upsample_cart, 0.7)
train_idx <-as.numeric(rownames(train))
test <- diabetes_upsample_cart[-train_idx,]
head(train)
head(test)

diabetes_train <- ranger(
  formula = y ~ .,
  data    = train,
  importance = "impurity"
  # xtest   = test$y,
  # ytest   = test[, 2:df_len]
)

diabetes_train$variable.importance
diabetes_train$prediction.error

print("Predict using the test set OF THE TRAIN SET")
diabetes_test <- predict(diabetes_train, data = test)
dim(diabetes_test$predictions)
CrossTable(test$y, diabetes_test$predictions)

print("Predict using the ORIGINAL TEST SET")
diabetes_true_test <- predict(diabetes_train, data = test_diabetes)
CrossTable(test_diabetes$DIABBC, diabetes_true_test$predictions)


diabetes_train$variable.importance %>% 
  tidy() %>%
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 25 important variables")

```

Based on the confusion matrix, the accuracy is higher than the null accuracy of the test set (~95%) that we withheld earlier.

```{r}

# Random Forest
(1531+59)/(1531+59+18+1)*100

```

In the training set, there were too many false negatives but in the test set this has considerably decreased. Thus in this particular experiment upsampling appears to have improved the results overall.  

```{r}

roc <- roc(response=as.numeric(test_diabetes$DIABBC), predictor=as.numeric(diabetes_true_test$predictions))
plot(roc)

roc

```

```{r, eval=FALSE, include=FALSE}

hyper_grid_2 <- expand.grid(
  num.trees = c(300, 400, 500),
  mtry       = seq(5, 14, by = 2),
  node_size  = seq(3, 8, by = 2),
  sampe_size = c(.55, .632, .70),
  OOB_RMSE  = 0
)

hyper_grid_2

# perform grid search
for(i in 1:nrow(hyper_grid_2)) {
  
  # train model
  model <- ranger(
    formula         = y ~ ., 
    data            = train, 
    importance      = "impurity",
    num.trees       = 500,
    mtry            = hyper_grid_2$mtry[i],
    min.node.size   = hyper_grid_2$node_size[i],
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid_2$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_2 %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)

```

### LDA and QDA

```{r}

X = data.frame(scale(diabetes_upsample[, 2:df_len]))
y = diabetes_upsample$DIABBC

cv_da(X = X, y = y, V = 10, method = "lda", seed = 1)
cv_da(X = X, y = y, V = 10, method = "qda", seed = 1)

```

### GLMs 

```{r}

X = data.frame(scale(diabetes_upsample[, 2:df_len]))
y = diabetes_upsample$DIABBC

mutate(diabetes_upsample, DIABBC = ifelse(DIABBC == "0", 0, 1))

logistic_regression = glm(y~., family=binomial, data=X)
summary(logistic_regression)

```

```{r, eval=FALSE, include=FALSE}

cv_glm(X, y, V = 10)
cv_glm_backward(X, y, V = 10, pen = log(df_samples))
cv_glm_backward(X, y, V = 10, pen = 2)

```

The results have improved dramatically, to the point where the random forest model has a 99% accuracy.

### Penalised Support Vector Machines

```{r}

train_diabetes <- sample_frac(diabetes_no_na, 0.7)
train_idx <-as.numeric(rownames(train))
test_diabetes <- train_diabetes[-train_idx,]
head(test_diabetes)

table(train_diabetes$DIABBC)

head(train_diabetes)

```

We first run an SVM before actually engaging with a test set.

```{r}

svm_model <- svm(DIABBC ~ ., data=train_diabetes, kernel="polynomial",
                 coef0=1,
                 gamma=0.1,
                 cost=0.1,
                 class.weights=c("0"=1, "1"=15)
                 )
summary(svm_model)

test_pred <- predict(svm_model, test_diabetes[, -1])
test_pred[1:10]
```

```{r}

CrossTable(test_diabetes$DIABBC, test_pred)

```


#### Hyperparameter Tuning

```{r}

svm_model_tuned <- tune(svm, DIABBC~., data = train_diabetes, kernel="radial", 
                 ranges = list(gamma=c(0.1,0.5,1,2,4), 
                               cost = c(0.1,1,10,100,1000)
                               ), 
                 class.weights= c("0" = 1, "1" = 15))

summary(svm_model_tuned)

```

```{r}

svm_model_tuned$best.parameters
svm_model_tuned$nparcomb

svm_model <- svm(DIABBC ~ ., data=train_diabetes, kernel="polynomial",
                 coef0=1,
                 gamma=2,
                 cost=0.1,
                 class.weights=c("0"=1, "1"=15)
                 )

test_pred <- predict(svm_model, test_diabetes[, -1])
test_pred[1:10]

CrossTable(test_diabetes$DIABBC, test_pred) 

```

```{r}

(1396+75)/(1396+137+2+75)*100

```

Comparatively, the penalised-SVM is did not score as highly in the overall accuracy especially compared to the Random Forest model. However, by weighting the classes differently, we were able to reduce the number of false negatives, despite the overall accuracy being 91%, slightly lower than the null accuracy.

All this analysis suggests that there are different methods of finetuning the FPR and FNR depending on your problem and these models all have their strengths and weaknesses. 