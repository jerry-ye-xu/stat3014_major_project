---
title: "Cholesterol"
author: "Jerry Xu"
date: "16 October 2018"
output: html_document
---

## Predicting Abnormal Levels of Cholesterol

Warning: The glm functions take a while to run. They have been commented out to save time when running the whole file.

```{r, include=FALSE}

# library(tidyverse)
library(tidyr)
library(dplyr)
library(broom) # for tidy()
library(ggplot2)
library(pROC)
library(plotROC)
library(gmodels) #CrossTable
library(MLmetrics)
library(ROCR)

library(e1071) # SVM

library(pscl)
library(caTools) # train_test_split

# library(rsample)      # data splitting - not available for the most recent version of R
library(randomForestSRC) # basic RF package
library(ranger)

source("./functions/cv_knn.R")
source("./functions/cv_da.R")
source("./functions/cv_rpart.R")
source("./functions/cv_glm.R")
source("./functions/cv_glm_backward.R")

```

Before doing anything, we'll set the seed to ensure reproducibility.

```{r}

set.seed(15)

```

## Standardised Preprocessing

```{r}

feature_variables = c("BMISC", "AGEC", "PHDKGWBC", "EXLWTBC", "SF2SA1QN", "INCDEC", "HSUGBC", "FATT1", "SUGART1", 
                       "PREVAT1", "PROVAT1", "FATPER1", "LAPER1", "ALAPER1", "CHOPER1", "SUGPER1", "SATPER1", "TRANPER1", 
                       "MONOPER1", "POLYPER1", "ADTOTSE", "SEX", "SMKSTAT", "SYSTOL", "FASTSTAD", "HDLCHREB", "LDLNTR", "LDLRESB", "B3T1")

# BMR, SLPTIME

response_variables = c("CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST")

all_variables = c(feature_variables, response_variables)

nutm_orig = read.csv("../output/nutmstat_factors_and_NAs.csv")
nutm = nutm_orig[all_variables]

```

We'll need to reprocess some of the variables as factors. 

```{r}
categoricalList <- c()

categoricalList[ 1 ] <- FALSE #  BMISC 
categoricalList[ 2 ] <- FALSE #  AGEC 

categoricalList[ 3 ] <- FALSE #  PHDKGWBC 

categoricalList[ 4 ] <- FALSE #  EXLWTBC 

categoricalList[ 5 ] <- TRUE #  SF2SA1QN 
categoricalList[ 6 ] <- TRUE #  INCDEC 

categoricalList[ 7 ] <- TRUE #  HSUGBC 

categoricalList[ 8 ] <- FALSE #  FATT1 

categoricalList[ 9 ] <- FALSE #  SUGART1 

categoricalList[ 10 ] <- FALSE #  PREVAT1 
categoricalList[ 11 ] <- FALSE #  PROVAT1 

categoricalList[ 12 ] <- FALSE #  FATPER1 
categoricalList[ 13 ] <- FALSE #  LAPER1 
categoricalList[ 14 ] <- FALSE #  ALAPER1 
categoricalList[ 15 ] <- FALSE #  CHOPER1 
categoricalList[ 16 ] <- FALSE #  SUGPER1 

categoricalList[ 17 ] <- FALSE #  SATPER1 
categoricalList[ 18 ] <- FALSE #  TRANPER1 

categoricalList[ 19 ] <- FALSE #  MONOPER1 
categoricalList[ 20 ] <- FALSE #  POLYPER1 
categoricalList[ 21 ] <- FALSE #  ADTOTSE 

categoricalList[ 22 ] <- TRUE #  SEX 

categoricalList[ 23 ] <- TRUE #  SMKSTAT 
categoricalList[ 24 ] <- FALSE #  SYSTOL 

categoricalList[ 25 ] <- TRUE #  FASTSTAD 

categoricalList[ 26 ] <- TRUE #  HDLCHREB 
categoricalList[ 27 ] <- TRUE #  LDLNTR 
categoricalList[ 28 ] <- TRUE #  LDLRESB 

categoricalList[ 29 ] <- FALSE #  B3T1

categoricalList[ 30 ] <- TRUE #  CHOLNTR 
categoricalList[ 31 ] <- TRUE #  HDLCHREB 
categoricalList[ 32 ] <- TRUE #  DIABBC 
categoricalList[ 33 ] <- TRUE #  HCHOLBC 
categoricalList[ 34 ] <- TRUE #  HYPBC 
categoricalList[ 35 ] <- TRUE #  CVDMEDST 

# "CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST"

for (i in 1:length(categoricalList)) {
  if (categoricalList[ i ]) {
      nutm[,i] <- as.factor(nutm[ ,i])
  }
}

head(nutm)
```

#### Data Preprocessing Stage

Let's firstly check how much data we have.

```{r}

sum(is.na(nutm["CHOLNTR"]))
table(nutm["CHOLNTR"])

```

We have a selection of variables that were filtered collectively by the group. Depending on the response variable. We shall subset these variables even further. 

```{r}

cholesterol_response = c("CHOLNTR")
cholesterol_variables = c("BMISC", "AGEC", "EXLWTBC", 
                       # "SF2SA1QN", "INCDEC",
                       # "HSUGBC", 
                       "FATT1", "SUGART1", 
                       "FATPER1", "LAPER1", "ALAPER1", "CHOPER1", 
                       "SATPER1", "TRANPER1", "MONOPER1", "POLYPER1", 
                       # "ADTOTSE",
                       # "SEX"
                       # "SMKSTAT", 
                       "SYSTOL", 
                       # "FASTSTAD", 
                       # "HDLCHREB", 
                       # "LDLNTR", 
                       # "LDLRESB", 
                       "B3T1"
                       )

cholesterol = nutm[c(cholesterol_response, cholesterol_variables)]
head(cholesterol)
cholesterol <- mutate(cholesterol, CHOLNTR = ifelse(CHOLNTR == 1, "0", "1"))
cholesterol$CHOLNTR <- as.factor(cholesterol$CHOLNTR)
# cholesterol$HDLCHREB <- as.numeric(cholesterol$HDLCHREB)
# cholesterol$LDLNTR <- as.numeric(cholesterol$LDLNTR)
# cholesterol$LDLRESB <- as.numeric(cholesterol$LDLRESB)
head(cholesterol)

```

We simply remove all rows were the null values exist. This does not cause us any major issues in both the size of the observed data and distortion of the proportion of the 2 classes.

```{r}

cholesterol_no_na <- cholesterol[complete.cases(cholesterol), ]
dim(cholesterol_no_na)

table_values <- table(cholesterol_no_na$CHOLNTR)

table_values[1]/sum(table_values)

```

We check the null accuracy, which is 63%. I am certain we can build a model to make substantial impovements on this baseline accuracy. 

Setting variables:

```{r}

df_len = dim(cholesterol_no_na)[2] # no. of columns in Dataframe 
df_len

neighbours = 3
V = 10 # cross-validation splits. 

```

Now we can try to fit a KNN to test the code. 

## Exploring Prediction with Baseline models 

### K-nearest Neighbours

```{r}

X = data.frame(scale(cholesterol_no_na[, 2:df_len]))
y = cholesterol_no_na$CHOLNTR
cv_knn(X, y, k=neighbours, V=10, seed=1)

k_neighbours = seq(1,21,by=2)
cv_errors = c()
for (i in 1:length(k_neighbours)) {
  cv_errors[i] = cv_knn(X, as.factor(y),k=k_neighbours[i],V,seed=1)
}
plot(k_neighbours, cv_errors, type="l", xlab="value of k", ylab="cross-validation error", cex.lab=1.5,
    main="CV errors for K-nearest neighbours", cex.main=2, ylim=c(0.2,0.60))

```

It seems like K-nearest neighbours performs even worse than the baseline. This is indeed baffling, which suggests that the 2 classes are close in the vector space and difficult to distinguish. 


We quickly calculate the ROC curve here for knn here, after splitting the samples 70/30.

```{r}

sample_sub = sample.split(cholesterol_no_na$CHOLNTR, SplitRatio = 0.7)
train = subset(cholesterol_no_na, sample_sub == TRUE)
test  = subset(cholesterol_no_na, sample_sub == FALSE)
head(train)
head(test)

train_X <- train[, 2:df_len]
test_X <-   test[, 2:df_len]

knn_split_test <- knn(train=train_X, test=test_X, cl=train$CHOLNTR , k = 5)
CrossTable(test$CHOLNTR, knn_split_test)

roc_knn <- roc(as.numeric(test$CHOLNTR), as.numeric(knn_split_test))
roc_knn
plot(roc_knn)
```

```{r}

F1_Score(test$CHOLNTR, knn_split_test, positive = NULL)

```

Let's try both the DA and CART algorithms as a sanity check. 

### LDA and QDA

```{r}

X = data.frame(scale(cholesterol_no_na[, 2:15]))
y = cholesterol_no_na$CHOLNTR

cv_da(X = X, y = y, V = 10, method = "lda", seed = 1)
cv_da(X = X, y = y, V = 10, method = "qda", seed = 1)

```

### CART

```{r}

X <- as.matrix(cholesterol_no_na[, 2:df_len])
y <- cholesterol_no_na$CHOLNTR
cholesterol_no_na_cart <- data.frame(y, X)

res_rpart <- rpart(y ~ X
, data=cholesterol_no_na)
rpart.plot(res_rpart,type=1,extra=1, main="CART for Cholesterol, non_zeroes")

summary(res_rpart)

print("The cross-validation accuracy is:")
cv_rpart(X, y, V, seed=1)

```

Both the KNN and CART algorithms fail to perform. We now turn Random Forests, which partitions the dataset in both samples and features - increasing bias slightly in compensation to a considerable reduction in variance. 

#### Improved Prediction with Random Forests

```{r}

X = data.frame(cholesterol_no_na[, 2:df_len])
y = cholesterol_no_na$CHOLNTR

cholesterol_no_na_cart <- data.frame(y, X)

sample_sub = sample.split(cholesterol_no_na$CHOLNTR, SplitRatio = 0.7)
train = subset(cholesterol_no_na, sample_sub == TRUE)
test  = subset(cholesterol_no_na, sample_sub == FALSE)
head(train)
head(test)

```

```{r}

chol_train <- ranger(
  formula = CHOLNTR ~ .,
  data    = train,
  importance = "impurity"
  # xtest   = test$y,
  # ytest   = test[, 2:df_len]
)

chol_train$variable.importance
chol_train$prediction.error

print("Predict using the test set")
chol_test <- predict(chol_train, data = test)

CrossTable(test$CHOLNTR, chol_test$predictions)

chol_train$variable.importance %>% 
  tidy() %>%
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 25 important variables")

```

Based on the confusion matrix, the accuracy is 

```{r}

table_results <- table(test$CHOLNTR, chol_test$predictions)

(table_results[1] + table_results[4])/(sum(table_results))*100

```

```{r}

F1_Score(test$CHOLNTR, chol_test$predictions, positive = NULL)

```


I want to check the parameters for the RF model

```{r}

summary(chol_train)

```


This is a significant improvement in prediction accuracy compared to the first 2 algorithms we used. The distribution of type I and II errors are also acceptable, but not surprisingly we have relatively more false negatives.  

```{r}

roc <- roc(response=as.numeric(chol_test$predictions), predictor=as.numeric(test$CHOLNTR))
auc <- auc(response=as.numeric(test$CHOLNTR), predictor=as.numeric(chol_test$predictions))
plot(roc)

roc
auc

```

__Tuning the Hyperparameters__

```{r, eval=TRUE, include=FALSE}

hyper_grid_2 <- expand.grid(
  mtry       = seq(1, 7, by = 2),
  node_size  = seq(1, 5, by = 2),
  sample_size = c(.55, .632, .70),
  CLASS_ACCURACY  = 0
)

# perform grid search
for(i in 1:nrow(hyper_grid_2)) {
  
  # train model
  model <- ranger(
    formula         = CHOLNTR ~ ., 
    data            = train, 
    write.forest    = TRUE,
    importance      = "impurity",
    num.trees       = 500,
    mtry            = hyper_grid_2$mtry[i],
    min.node.size   = hyper_grid_2$node_size[i],
    sample.fraction = hyper_grid_2$sample_size[i],
    seed            = 123
  )
  
  diabetes_test <- predict(model, data = test)
  c_m <- table(test$CHOLNTR, diabetes_test$predictions)[1:4]
  # add OOB error to grid
  classification_accuracy_for_test_set <- (c_m[1]+c_m[4])/sum(c_m)
  
  hyper_grid_2$CLASS_ACCURACY[i] <- classification_accuracy_for_test_set
}

hyper_grid_2 %>% 
  dplyr::arrange(CLASS_ACCURACY) %>%
  head(10)

```

```{r, eval=TRUE, include=FALSE}

tuning_results <- hyper_grid_2[order(hyper_grid_2$CLASS_ACCURACY, decreasing=TRUE), ]
head(tuning_results)
optimal_parameters_idx <-rownames(tuning_results)[1]

```

```{r, eval=TRUE, include=FALSE}

sample_sub = sample.split(cholesterol_no_na_cart$y, SplitRatio = 0.7)
train = subset(cholesterol_no_na_cart, sample_sub == TRUE)
test  = subset(cholesterol_no_na_cart, sample_sub == FALSE)
head(train)
head(test)


diabetes_train <- ranger(
  formula         = y ~ .,
  data            = train,
  importance      = "impurity",
  num.trees       = 500,
  mtry            = hyper_grid_2[optimal_parameters_idx, "mtry"],
  min.node.size   = hyper_grid_2[optimal_parameters_idx, "node_size"],
  sample.fraction = hyper_grid_2[optimal_parameters_idx, "sample_size"]
)

diabetes_train$variable.importance
diabetes_train$prediction.error

print("Predict using the test set")
diabetes_test <- predict(diabetes_train, data = test)

CrossTable(test$y, diabetes_test$predictions)

```

#### GLMs


__Analysing the Coefficients__

```{r, include=FALSE}

X = data.frame(cholesterol_no_na[, 2:df_len])
y = cholesterol_no_na$CHOLNTR

mutate(cholesterol_no_na, CHOLNTR = ifelse(CHOLNTR == "0", 0, 1))

```

```{r}

logistic_regression = glm(y~., family=binomial, data=X)
summary(logistic_regression)

anova(logistic_regression)
pR2(logistic_regression) # Look at McFadden

```

```{r}

cv_glm(X, y, V = 10)

```


__Testing Prediction Capabilities for Logistic Regression__

Now let's test how well this model performs. 

```{r}

head(test)

```


```{r}

sample_sub = sample.split(cholesterol_no_na$CHOLNTR, SplitRatio = 0.7)
train = subset(cholesterol_no_na, sample_sub == TRUE)
test  = subset(cholesterol_no_na, sample_sub == FALSE)
head(train)
head(test)

prob_threshold = 0.45

chol_test_pred <- predict(logistic_regression, newdata=test, type='response')
chol_test_pred <- ifelse(chol_test_pred > prob_threshold, 1, 0)

CrossTable(test$CHOLNTR, chol_test_pred)

misClasificError <- mean(chol_test_pred != test$CHOLNTR)
print(paste('Accuracy: ',1-misClasificError))

print(paste("F1 score: ", F1_Score(test$CHOLNTR, chol_test_pred, positive=0)))

```

```{r}

pr <- prediction(chol_test_pred, test$CHOLNTR)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc


```

Logistic regression performs rather poorly, with tuning of the threshold probability p leading to no substantial improvement in both the classification accuracy nor the overall F1_score. 

It seems that Random Forest is vastly superior to the other classification algorithms for this dataset. 

Now let's try to conduct some cross-validation on GLMs. 

```{r, eval=FALSE, include=FALSE}

cv_glm_backward(X, y, V = 10, pen = log(df_samples))
cv_glm_backward(X, y, V = 10, pen = 2)

```

## Support Vector Machines

```{r}

class_weights = c(1, 17)

sample_sub = sample.split(cholesterol_no_na$CHOLNTR, SplitRatio = 0.7)
train = subset(cholesterol_no_na, sample_sub == TRUE)
test  = subset(cholesterol_no_na, sample_sub == FALSE)
head(train)
head(test)

svm_model <- svm(CHOLNTR ~ ., data=train, kernel="polynomial",
                 coef0=1,
                 gamma=0.1,
                 cost=0.1,
                 class.weights=c("0"=class_weights[1], "1"=class_weights[2])
                 )
summary(svm_model)

test_pred <- predict(svm_model, test[, -1])
test_pred[1:10]

```

```{r}

CrossTable(test$CHOLNTR, test_pred)

table_results <- table(test$CHOLNTR, test_pred)
print("Accuracy:")
(table_results[1]+table_results[2])/sum(table_results)*100

print("F1_score:")
F1_Score(test$CHOLNTR, test_pred)

```