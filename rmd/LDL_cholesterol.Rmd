---
title: "LDL_cholesterol"
author: "Jerry Xu"
date: "18 October 2018"
output: html_document
---

## Predicting Abnormal Levels of LDL Cholesterol

Warning: The glm functions take a while to run. They have been commented out to save time when running the whole file.

```{r, include=TRUE}

# library(tidyverse)
library(tidyr)
library(dplyr)
library(broom) # for tidy()
library(ggplot2)
library(pROC)
library(plotROC)
library(gmodels) #CrossTable
library(MLmetrics)
library(ROCR)

library(e1071) # SVM

library(pscl)
library(caTools) # train_test_split

# library(rsample)      # data splitting - not available for the most recent version of R
library(randomForestSRC) # basic RF package
library(ranger)

source("./functions/cv_knn.R")
source("./functions/cv_da.R")
source("./functions/cv_rpart.R")
source("./functions/cv_glm.R")
source("./functions/cv_glm_backward.R")

```

## Standardised Preprocessing

```{r}

feature_variables = c("BMISC", "AGEC", "PHDKGWBC", "EXLWTBC", "SF2SA1QN", "INCDEC", "HSUGBC", "FATT1", "SUGART1", 
                       "PREVAT1", "PROVAT1", "FATPER1", "LAPER1", "ALAPER1", "CHOPER1", "SUGPER1", "SATPER1", "TRANPER1", 
                       "MONOPER1", "POLYPER1", "ADTOTSE", "SEX", "SMKSTAT", "SYSTOL", "FASTSTAD", "HDLCHREB", "LDLNTR", "LDLRESB", "B3T1")

# BMR, SLPTIME

response_variables = c("CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST")

all_variables = c(feature_variables, response_variables)

nutm_orig = read.csv("../output/nutmstat_factors_and_NAs.csv")
nutm = nutm_orig[all_variables]

```

We'll need to reprocess some of the variables as factors. 

```{r}
categoricalList <- c()

categoricalList[ 1 ] <- FALSE #  BMISC 
categoricalList[ 2 ] <- FALSE #  AGEC 

categoricalList[ 3 ] <- FALSE #  PHDKGWBC 

categoricalList[ 4 ] <- FALSE #  EXLWTBC 

categoricalList[ 5 ] <- TRUE #  SF2SA1QN 
categoricalList[ 6 ] <- TRUE #  INCDEC 

categoricalList[ 7 ] <- TRUE #  HSUGBC 

categoricalList[ 8 ] <- FALSE #  FATT1 

categoricalList[ 9 ] <- FALSE #  SUGART1 

categoricalList[ 10 ] <- FALSE #  PREVAT1 
categoricalList[ 11 ] <- FALSE #  PROVAT1 

categoricalList[ 12 ] <- FALSE #  FATPER1 
categoricalList[ 13 ] <- FALSE #  LAPER1 
categoricalList[ 14 ] <- FALSE #  ALAPER1 
categoricalList[ 15 ] <- FALSE #  CHOPER1 
categoricalList[ 16 ] <- FALSE #  SUGPER1 

categoricalList[ 17 ] <- FALSE #  SATPER1 
categoricalList[ 18 ] <- FALSE #  TRANPER1 

categoricalList[ 19 ] <- FALSE #  MONOPER1 
categoricalList[ 20 ] <- FALSE #  POLYPER1 
categoricalList[ 21 ] <- FALSE #  ADTOTSE 

categoricalList[ 22 ] <- TRUE #  SEX 

categoricalList[ 23 ] <- TRUE #  SMKSTAT 
categoricalList[ 24 ] <- FALSE #  SYSTOL 

categoricalList[ 25 ] <- TRUE #  FASTSTAD 

categoricalList[ 26 ] <- TRUE #  HDLCHREB 
categoricalList[ 27 ] <- TRUE #  LDLNTR 
categoricalList[ 28 ] <- TRUE #  LDLRESB 

categoricalList[ 29 ] <- FALSE #  B3T1

categoricalList[ 30 ] <- TRUE #  CHOLNTR 
categoricalList[ 31 ] <- TRUE #  HDLCHREB 
categoricalList[ 32 ] <- TRUE #  DIABBC 
categoricalList[ 33 ] <- TRUE #  HCHOLBC 
categoricalList[ 34 ] <- TRUE #  HYPBC 
categoricalList[ 35 ] <- TRUE #  CVDMEDST 

# "CHOLNTR", "HDLCHREB", "DIABBC", "HCHOLBC", "HYPBC", "CVDMEDST"

for (i in 1:length(categoricalList)) {
  if (categoricalList[ i ]) {
      nutm[,i] <- as.factor(nutm[ ,i])
  }
}

head(nutm)
```

#### Data Preprocessing Stage

Let's firstly check how much data we have.

```{r}

sum(is.na(nutm["LDLNTR"]))
table(nutm["LDLNTR"])

```

We have a selection of variables that were filtered collectively by the group. Depending on the response variable. We shall subset these variables even further. 

```{r}

cholesterol_response = c("LDLNTR")
cholesterol_variables = c("BMISC", "AGEC", "EXLWTBC", 
                       # "SF2SA1QN", "INCDEC",
                       # "HSUGBC", 
                       "FATT1", "SUGART1", 
                       "FATPER1", "LAPER1", "ALAPER1", "CHOPER1", 
                       "SATPER1", "TRANPER1", "MONOPER1", "POLYPER1", 
                       # "ADTOTSE",
                       # "SEX"
                       # "SMKSTAT", 
                       "SYSTOL", 
                       # "FASTSTAD", 
                       # "HDLCHREB", 
                       # "LDLNTR", 
                       # "LDLRESB", 
                       "B3T1"
                       )

cholesterol = nutm[c(cholesterol_response, cholesterol_variables)]

cholesterol <- mutate(cholesterol, LDLNTR = ifelse(LDLNTR == 1, "0", "1"))
cholesterol$LDLNTR <- as.factor(cholesterol$LDLNTR)
# cholesterol$HDLCHREB <- as.numeric(cholesterol$HDLCHREB)
# cholesterol$LDLNTR <- as.numeric(cholesterol$LDLNTR)
# cholesterol$LDLRESB <- as.numeric(cholesterol$LDLRESB)
head(cholesterol)

```

We simply remove all rows were the null values exist. This does not cause us any major issues in both the size of the observed data and distortion of the proportion of the 2 classes.

```{r}

cholesterol_no_na <- cholesterol[complete.cases(cholesterol), ]
dim(cholesterol_no_na)

table(cholesterol_no_na$LDLNTR)

1873/(1051+1873)

```

We check the null accuracy, which is 64%. I am certain we can build a model to make substantial impovements on this baseline accuracy. 

Setting variables:

```{r}

df_len = dim(cholesterol_no_na)[2] # no. of columns in Dataframe 
df_len

neighbours = 3
V = 10 # cross-validation splits. 

```


```{r}

X = data.frame(scale(cholesterol_no_na[, 2:df_len]))
y = cholesterol_no_na$LDLNTR

cv_knn(X, y, k=neighbours, V=10, seed=1)

k_neighbours = seq(1,15,by=2)
cv_errors = c()
for (i in 1:length(k_neighbours)) {
  cv_errors[i] = cv_knn(X, as.factor(y),k=k_neighbours[i],V,seed=1)
}
plot(k_neighbours, cv_errors, type="l", xlab="value of k", ylab="cross-validation error", cex.lab=1.5,
    main="CV errors for K-nearest neighbours", cex.main=2, ylim=c(0.2,0.60))

```

It seems like K-nearest neighbours performs even worse than the baseline. This is indeed baffling, which suggests that the 2 classes are close in the vector space and difficult to distinguish. 

### LDA and QDA

```{r}

X = data.frame(scale(cholesterol_no_na[, 2:15]))
y = cholesterol_no_na$LDLNTR

cv_da(X = X, y = y, V = 10, method = "lda", seed = 1)
cv_da(X = X, y = y, V = 10, method = "qda", seed = 1)

```

```{r}

X <- as.matrix(cholesterol_no_na[, 2:df_len])
y <- cholesterol_no_na$LDLNTR
cholesterol_no_na_cart <- data.frame(y, X)

res_rpart <- rpart(y ~ X
, data=cholesterol_no_na,  method="class")
rpart.plot(res_rpart,type=1,extra=1, main="CART for LDL Cholesterol, non_zeroes")

summary(res_rpart)

print("The cross-validation accuracy is:")
cv_rpart(X, y, V, seed=1)

```

Both the KNN and CART algorithms fail to perform. We now turn Random Forests, which partitions the dataset in both samples and features - increasing bias slightly in compensation to a considerable reduction in variance. 

#### Random Forests

```{r}

X = data.frame(cholesterol_no_na[, 2:df_len])
y = cholesterol_no_na$LDLNTR

cholesterol_no_na_cart <- data.frame(y, X)

sample_sub = sample.split(cholesterol_no_na_cart$y, SplitRatio = 0.7)
train = subset(cholesterol_no_na_cart, sample_sub == TRUE)
test  = subset(cholesterol_no_na_cart, sample_sub == FALSE)
head(train)
head(test)

```

```{r}

chol_train <- ranger(
  formula = y ~ .,
  data    = train,
  importance = "impurity"
  # xtest   = test$y,
  # ytest   = test[, 2:df_len]
)

chol_train$variable.importance
chol_train$prediction.error

print("Predict using the test set")
chol_test <- predict(chol_train, data = test)

CrossTable(test$y, chol_test$predictions)

chol_train$variable.importance %>% 
  tidy() %>%
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 25 important variables")

```

Based on the confusion matrix, the accuracy is 

```{r}

table_results <- table(test$y, chol_test$predictions)
(table_results[1] + table_results[4])/sum(table_results)*100

```

This is a significant improvement in prediction accuracy compared to the first 2 algorithms we used. The distribution of type I and II errors are also acceptable, but not surprisingly we have relatively more false negatives.  

```{r}

roc <- roc(response=as.numeric(chol_test$predictions), predictor=as.numeric(test$y))
auc <- auc(response=as.numeric(test$y), predictor=as.numeric(chol_test$predictions))
plot(roc)

roc
auc

```

__Tuning the Hyperparameters__

```{r, eval=FALSE, include=FALSE}

hyper_grid_2 <- expand.grid(
  mtry       = seq(5, 14, by = 2),
  node_size  = seq(3, 8, by = 2),
  sampe_size = c(.55, .632, .70),
  OOB_RMSE  = 0
)

hyper_grid_2

# perform grid search
for(i in 1:nrow(hyper_grid_2)) {
  
  # train model
  model <- ranger(
    formula         = y ~ ., 
    data            = train, 
    importance      = "impurity",
    num.trees       = 500,
    mtry            = hyper_grid_2$mtry[i],
    min.node.size   = hyper_grid_2$node_size[i],
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid_2$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_2 %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)

```

#### GLMs

```{r, include=TRUE}

X = data.frame(cholesterol_no_na[, 2:df_len])
y = cholesterol_no_na$LDLNTR

# mutate(cholesterol_no_na, LDLNTR = ifelse(LDLNTR == "0", 0, 1))

logistic_regression = glm(y~., family=binomial, data=X)
summary(logistic_regression)

```

```{r}

anova(logistic_regression)
pR2(logistic_regression) # Look at McFadden

```

```{r}

cv_glm(X, y, V = 10)

```


__Testing Prediction Capabilities for Logistic Regression__

Now let's test how well this model performs.

```{r}

sample_sub = sample.split(cholesterol_no_na_cart$y, SplitRatio = 0.7)
train = subset(cholesterol_no_na_cart, sample_sub == TRUE)
test  = subset(cholesterol_no_na_cart, sample_sub == FALSE)
head(train)
head(test)

prob_threshold = 0.375

chol_test_pred <- predict(logistic_regression, newdata=test, type='response')
chol_test_pred <- ifelse(chol_test_pred > prob_threshold, "1", "0")

CrossTable(test$y, chol_test_pred)

misClasificError <- mean(chol_test_pred != test$y)
print(paste('Accuracy: ',1-misClasificError))

print(paste("F1 score: ", F1_Score(test$y, chol_test_pred)))

```

Now let's try to conduct some cross-validation on GLMs. 

```{r, eval=FALSE, include=FALSE}

cv_glm_backward(X, y, V = 10, pen = log(df_samples))
cv_glm_backward(X, y, V = 10, pen = 2)

```

## Support Vector Machines

```{r}
cholesterol_no_na
train
```

```{r}
cholesterol_no_na
```


```{r}

class_weights = c(1, 2.25)

#cholesterol_no_na <- mutate(cholesterol_no_na, LDLNTR = ifelse(LDLNTR == "1", 1, 0))

sample_sub = sample.split(cholesterol_no_na$LDLNTR, SplitRatio = 0.7)
train = subset(cholesterol_no_na, sample_sub == TRUE)
test  = subset(cholesterol_no_na, sample_sub == FALSE)
head(train)
head(test)

svm_model <- svm(LDLNTR ~ ., data=train, kernel="polynomial",
                 coef0=1,
                 gamma=0.1,
                 cost=0.1,
                 class.weights=c("0"=class_weights[1], "1"=class_weights[2])
                 )
summary(svm_model)

test_pred <- predict(svm_model, test[, -1])
test_pred[1:10]

```

```{r}

CrossTable(test$LDLNTR, test_pred)

table_results <- table(test$LDLNTR, test_pred)

print("Accuracy:")
(table_results[1]+table_results[4])/sum(table_results)*100

print("F1_score:")
F1_Score(test$LDLNTR, test_pred)

```

